{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bc0109-9953-4d52-8688-5d9a1d8ff850",
   "metadata": {},
   "source": [
    "# Using DistilBert for SA on the Sentiment140 Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db09150-36de-49d3-8af4-d2e4d5c82e56",
   "metadata": {},
   "source": [
    "Import Statements - also, setting random seed for reproducibility and some plot settings for seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9ea11b-567f-47fa-984c-499ec1aac06a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#DistilBert + Tokenizer\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "#train/test/dev split and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2345883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random seed + styling\n",
    "\n",
    "SEED=0\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "plt.style.use(\"classic\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
    "\n",
    "#tqdm progress bar for pandas methods\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03e983e-3991-4b9b-9528-0ba014a9fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c09602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#check if using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b37864-909b-456a-8c04-9739a48a3980",
   "metadata": {},
   "source": [
    "### Final Preprocessing\n",
    "\n",
    "As we will be training this model (as opposed to using an out-of-the-box solution as seen in sent_flair.ipynb, sent_nltk.ipynb, etc.), there will be a few extra steps in regards to preprocessing:\n",
    "\n",
    "Loading dataset, splitting into train, val and test, tokenizing with DistilBert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426835d6-48fd-4974-92df-1fee543fc0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_clean.csv', sep='\\t', usecols=['sent', 'text', 'data_len', 'token_lens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e172451-c2f0-4336-98d2-a29395b461fd",
   "metadata": {},
   "source": [
    "Splitting the dataframe into test, val and test sets.  Test is 0.7, Val and Test are both 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f49afd4-57ca-4aa8-a25b-ef4fd77ea189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, temp = train_test_split(df, test_size=0.3)\n",
    "df_val, df_test = train_test_split(temp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1942354-d349-4f88-8f4f-cdd975350ae7",
   "metadata": {},
   "source": [
    "Confirming that we properly split data by looking at the shapes of the new datasets.  Also head of train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0fa7e8c-1dc6-4b56-8bf3-ce813028d98a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078253, 4)\n",
      "(231054, 4)\n",
      "(231055, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89f92d",
   "metadata": {},
   "source": [
    "Creating y_train, y_val and y_test from the ['sent'] column of their respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248e1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['sent']\n",
    "y_val = df_val['sent']\n",
    "y_test = df_test['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524e24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339633d-d100-4b0e-a6e7-9da963947769",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5453f87-73a6-48b9-92f7-b5ecddba8cdc",
   "metadata": {},
   "source": [
    "Finally, the various preprocessing steps are over.\n",
    "The code below:\n",
    "1) Initializes the DistilBert tokenizer and defines the tokenize() function\n",
    "2) Tokenizes the train, val, and test data in turn\n",
    "3) Trains the DistilBert model on the train and val data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09763762",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9d23b3-2203-41a1-adfb-b582e59a3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize(data,max_len=MAX_LEN) :\n",
    "\n",
    "    input_ids = []\n",
    "\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length = max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "        \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892ebaf-64eb-48f9-8c95-f518724826b1",
   "metadata": {},
   "source": [
    "Creating arrays containing input_ids and attention masks as returned by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2c9d56-ef95-4a3b-933a-0cb9a7e0d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1078253/1078253 [04:53<00:00, 3668.97it/s]\n",
      "100%|██████████| 231054/231054 [01:05<00:00, 3551.66it/s]\n",
      "100%|██████████| 231055/231055 [01:18<00:00, 2943.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks = tokenize(df_train.text.values)\n",
    "val_input_ids, val_attention_masks = tokenize(df_val.text.values)\n",
    "test_input_ids, test_attention_masks = tokenize(df_test.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2db257-388a-471d-93e6-f7ff1caba9e4",
   "metadata": {},
   "source": [
    "Model creation - setting the optimizer, loss func, and accuracy metric.  Model comprises of two input layers, one taking the input_ids, the other the corresponding attention mask.  These are fed into the bertlike model - which in this case DistilBert.  Take the hidden state of the cls token as a representation of the sentences sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f1f058-f5e9-46b2-82f2-9bd5991f54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-5 #1e-4, 3e-4, 5e-5, 3e-5\n",
    "N_EPOCHS = 3\n",
    "BATCH_SIZE = 64 #8, 16, 32, 64, 128\n",
    "\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ec565",
   "metadata": {},
   "source": [
    "After the model creation above, we display the models summary.  We can see everything seems to be connected up nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6a6d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e57e5c",
   "metadata": {},
   "source": [
    "Here is the portion that takes by far the longest - model training.\n",
    "\n",
    "With 10gb of VRAM on my home machine, each epoch (training data is ~1 million tweets) takes around 90 minutes.  So training anywhere from 2 to 4 epochs, expect training times of around 3-6 hours with a similar machine.\n",
    "\n",
    "This should be less of a problem if running on better hardware or through a cloud platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e32a6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 1751/16848 [==>...........................] - ETA: 1:21:01 - loss: 0.4185 - accuracy: 0.8084"
     ]
    }
   ],
   "source": [
    "model.fit([train_input_ids, train_attention_masks], y_train, validation_data=([val_input_ids, val_attention_masks], y_val), batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ce90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to save model\n",
    "\n",
    "model.save_pretrained('trained_distilbert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9449fd",
   "metadata": {},
   "source": [
    "Below is the final evaluation of the model on `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2f0db-df90-4ad5-8e11-8ca71cd93c13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'}), <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate([test_input_ids,test_attention_masks], y_test, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\thel0\\OneDrive\\Desktop\\Sent Analysis\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'}), <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "results = model.evaluate([test_input_ids,test_attention_masks], y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37611e1b-aeef-4248-a6af-042a8e2285c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36584365367889404, 0.8491830825805664]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed216af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to save df_test\n",
    "\n",
    "df_test.to_csv('data_test.csv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
